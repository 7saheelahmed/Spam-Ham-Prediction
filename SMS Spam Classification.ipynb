{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMS Spam Classifier \n",
    "* This dataset is a part of [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection).\n",
    "* This dataset contain collections of messages labelled as **spam** and **ham**.\n",
    "* The purpose of the project is to create a machine learning classifier to classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "df = pd.read_csv('sms_spam.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Dataset Specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5574, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# printing the shape of dataset. It has over 5k rows and 2 columns.\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     4827\n",
       "spam     747\n",
       "Name: type, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# two type of sms's \"ham\" and \"spam\"\n",
    "df.type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 13.40% spam and 86.60% of ham.\n"
     ]
    }
   ],
   "source": [
    "spam_percent = (len(df[df.type == 'spam'])/len(df))*100\n",
    "ham_percent = (len(df[df.type=='ham'])/len(df))*100\n",
    "print(\"The dataset has {0:.2f}% spam and {1:.2f}% of ham.\".format(spam_percent,ham_percent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It could be observed that the dataset is **highly skewed**.Biased towards one label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Go until jurong point, crazy.. Available only ...\n",
       "1                        Ok lar... Joking wif u oni...\n",
       "2    Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3    U dun say so early hor... U c already then say...\n",
       "4    Nah I don't think he goes to usf, he lives aro...\n",
       "5    FreeMsg Hey there darling it's been 3 week's n...\n",
       "6    Even my brother is not like to speak with me. ...\n",
       "7    As per your request 'Melle Melle (Oru Minnamin...\n",
       "8    WINNER!! As a valued network customer you have...\n",
       "9    Had your mobile 11 months or more? U R entitle...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4827\n",
       "1     747\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# putting labels 1 for spam and 0 for ham\n",
    "df['label'] = df['type'].map(lambda x : 1 if x=='spam' else 0)\n",
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#droping the \"type\" column\n",
    "df.drop('type', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Making train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], \n",
    "                                                    df['label'], \n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train first entry:\n",
      "\n",
      " The whole car appreciated the last two! Dad and are having a map reading semi argument but apart from that things are going ok. P.\n",
      "\n",
      "\n",
      "X_train shape:  (4180,)\n",
      "\n",
      "\n",
      "X_train shape:  (1394,)\n"
     ]
    }
   ],
   "source": [
    "print('X_train first entry:\\n\\n', X_train.iloc[0])\n",
    "print('\\n\\nX_train shape: ', X_train.shape)\n",
    "print('\\n\\nX_train shape: ', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It is basically a **75-25 %** split. 75% goes to training set & 25% test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Before training our model we need to preprocess certain features from our dataset. One of them being the ''uppercase-lowercase'' issue, frequency count of words in a spam or ham, and the distribution of important words ignoring words like \"the\" , \"a\" ,\"an\" etc.\n",
    "\n",
    "\n",
    "\n",
    "* For that we use an approach known as **bag of word** approach. The bag of words approach is commonly used way to represent text for use in machine learning which ignores structures and only counts how often each word occurs . It allows us to use bags of words approach by converting a collections of text document into matrix of token count.\n",
    "\n",
    "\n",
    "\n",
    "* Sklearn has a method known as **CountVectorisor**. Fitting the count vectorisor consist of the tokenisation of the trained data and building of the vocabulary. It fits by finding all sequnces of characters of atleast two letters or numbers separated by word boundaries.Convert everything to lowercase and builds a vocabulary using these tokens. \n",
    "\n",
    "\n",
    "* The result of CountVectorisor is a **high dimensional sparse matrix** containing count of each word in the dataset stored in a  compressed row format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using CountVectoriser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Fit the CountVectorizer to the training data\n",
    "vect = CountVectorizer().fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00', 'areyouunique', 'crowd', 'gently', 'lion', 'petey', 'sittin', 'uworld']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.get_feature_names()[::1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4180x7490 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 55394 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform the documents in the training data to a document-term matrix\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "\n",
    "X_train_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# Train the model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.9345550407141154\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Predict the transformed test documents\n",
    "predictions = model.predict(vect.transform(X_test))\n",
    "\n",
    "print('AUC: ', roc_auc_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------Classification Report------------------------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99      1198\n",
      "          1       0.98      0.87      0.92       196\n",
      "\n",
      "avg / total       0.98      0.98      0.98      1394\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(\"\\n----------Classification Report------------------------------------\")\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP 171\n",
      "TN 1194\n",
      "FP 4\n",
      "FN 25\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x216 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import confusion_matrix,recall_score,precision_recall_curve,auc,roc_curve,roc_auc_score,classification_report\n",
    "\n",
    "cnf_matrix=confusion_matrix(y_test,predictions)\n",
    "fig= plt.figure(figsize=(6,3))\n",
    "print(\"TP\",cnf_matrix[1,1,]) # TP denotes True Positive\n",
    "print(\"TN\",cnf_matrix[0,0]) # TN denotes True Negative\n",
    "print(\"FP\",cnf_matrix[0,1]) # FP denotes False Positive\n",
    "print(\"FN\",cnf_matrix[1,0]) # FN denotes False Negative\n",
    "sns.heatmap(cnf_matrix,cmap=\"coolwarm_r\",annot=True,linewidths=0.5)\n",
    "plt.title(\"Confusion_matrix\")\n",
    "plt.xlabel(\"Predicted_class\")\n",
    "plt.ylabel(\"True class\")\n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs:\n",
      "['me' 'gt' 'my' 'lt' 'll' 'what' 'then' 'but' 'da' 'good']\n",
      "\n",
      "Largest Coefs: \n",
      "['txt' 'text' 'uk' 'call' 'won' 'free' 'ringtone' 'reply' 'mobile' 'new']\n"
     ]
    }
   ],
   "source": [
    "# get the feature names as numpy array\n",
    "feature_names = np.array(vect.get_feature_names())\n",
    "\n",
    "# Sort the coefficients from the model\n",
    "sorted_coef_index = model.coef_[0].argsort()\n",
    "\n",
    "# Find the 10 smallest and 10 largest coefficients\n",
    "# The 10 largest coefficients are being indexed using [:-11:-1] \n",
    "# so the list returned is in order of largest to smallest\n",
    "print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "print('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using TF-IDF Vectorisor\n",
    "\n",
    "* The term frequency inverse document frequency allows us to weight terms based on how important they are to the document.\n",
    "\n",
    "* High weight is given to terms that appear often in a particular document but doesn't appear often in the corpus. Features with low TF-IDF are either commonly used across all document or rarely used and only in long document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1469"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Fit the TfidfVectorizer to the training data specifiying a minimum document frequency of 5\n",
    "vect = TfidfVectorizer(min_df=5).fit(X_train)\n",
    "len(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.8941560764539539\n"
     ]
    }
   ],
   "source": [
    "X_train_vectorized = vect.transform(X_train)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "predictions = model.predict(vect.transform(X_test))\n",
    "\n",
    "print('AUC: ', roc_auc_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------Classification Report------------------------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.98      1198\n",
      "          1       0.98      0.79      0.88       196\n",
      "\n",
      "avg / total       0.97      0.97      0.97      1394\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n----------Classification Report------------------------------------\")\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest tfidf:\n",
      "['norm150p' '36504' 'w45wq' 'hg' 'rays' 'ended' 'moral' '08000930705'\n",
      " 'camcorder' 'cr9']\n",
      "\n",
      "Largest tfidf: \n",
      "['nite' 'of' 'babe' 'today' 'home' 'too' 'type' 'alrite' 'for' 'anytime']\n"
     ]
    }
   ],
   "source": [
    "feature_names = np.array(vect.get_feature_names())\n",
    "\n",
    "sorted_tfidf_index = X_train_vectorized.max(0).toarray()[0].argsort()\n",
    "\n",
    "print('Smallest tfidf:\\n{}\\n'.format(feature_names[sorted_tfidf_index[:10]]))\n",
    "print('Largest tfidf: \\n{}'.format(feature_names[sorted_tfidf_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs:\n",
      "['me' 'my' 'ok' 'gt' 'that' 'lt' 'll' 'can' 'how' 'what']\n",
      "\n",
      "Largest Coefs: \n",
      "['txt' 'call' 'text' 'free' 'mobile' 'to' 'uk' 'www' 'stop' 'claim']\n"
     ]
    }
   ],
   "source": [
    "sorted_coef_index = model.coef_[0].argsort()\n",
    "\n",
    "print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "print('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using TF-IDF with N-grams\n",
    "\n",
    "* It adds sequences of word features known as bigrams , which count pair of adjacent words could be given as features like working versus not working . A trigrams which gives us triplets of adjacent words ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2863"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the CountVectorizer to the training data specifiying a minimum \n",
    "# document frequency of 5 and extracting 1-grams and 2-grams\n",
    "vect = CountVectorizer(min_df=5, ngram_range=(1,2)).fit(X_train)\n",
    "\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "\n",
    "len(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.9320040203059521\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "predictions = model.predict(vect.transform(X_test))\n",
    "\n",
    "print('AUC: ', roc_auc_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------Classification Report------------------------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99      1198\n",
      "          1       0.98      0.87      0.92       196\n",
      "\n",
      "avg / total       0.98      0.98      0.98      1394\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n----------Classification Report------------------------------------\")\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs:\n",
      "['me' 'my' 'gt' 'lt' 'll' 'lt gt' 'then' 'what' 'da' 'he']\n",
      "\n",
      "Largest Coefs: \n",
      "['txt' 'text' 'free' 'uk' 'you have' 'reply' 'call' 'won' 'ringtone'\n",
      " 'claim']\n"
     ]
    }
   ],
   "source": [
    "feature_names = np.array(vect.get_feature_names())\n",
    "\n",
    "sorted_coef_index = model.coef_[0].argsort()\n",
    "\n",
    "print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "print('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
